{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including os for file handling, nltk for text processing, and pandas for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing required libraries\n",
    "\n",
    "# os library for file handling\n",
    "import os\n",
    "\n",
    "# nltk library for text processing\n",
    "import nltk\n",
    "\n",
    "# pandas library for data manipulation\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Documents\n",
    "Load all documents from the data folder and preprocess them by removing stop words, stemming, and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing additional required libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Defining the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Defining the set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing stop words and stemming\n",
    "    processed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "# Loading and preprocessing documents\n",
    "documents = {}\n",
    "for filename in os.listdir('data'):\n",
    "    filepath = os.path.join('data', filename)\n",
    "    if os.path.isfile(filepath):  # Check if the path is a file\n",
    "        with open(filepath, 'r') as file:\n",
    "            text = file.read()\n",
    "            documents[filename] = preprocess_text(text)\n",
    "\n",
    "# Converting the dictionary to a pandas DataFrame\n",
    "documents_df = pd.DataFrame(list(documents.items()), columns=['Filename', 'Processed Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Inverted Index\n",
    "Create an inverted index that maps each word to the documents that contain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing defaultdict from collections\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to create inverted index\n",
    "def create_inverted_index(documents_df):\n",
    "    # Initializing the inverted index as a defaultdict of sets\n",
    "    inverted_index = defaultdict(set)\n",
    "    \n",
    "    # Iterating over each document\n",
    "    for index, row in documents_df.iterrows():\n",
    "        # Iterating over each word in the document\n",
    "        for word in row['Processed Text']:\n",
    "            # Adding the document to the set of documents containing the word\n",
    "            inverted_index[word].add(row['Filename'])\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "# Creating the inverted index\n",
    "inverted_index = create_inverted_index(documents_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Scoring Algorithm\n",
    "Define a scoring algorithm, such as TF-IDF or BM25, to rank the documents based on their relevance to the search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing additional required libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to calculate TF-IDF scores\n",
    "def calculate_tfidf_scores(documents_df):\n",
    "    # Initializing the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Calculating TF-IDF scores\n",
    "    tfidf_scores = vectorizer.fit_transform([' '.join(text) for text in documents_df['Processed Text']])\n",
    "\n",
    "    return tfidf_scores, vectorizer\n",
    "\n",
    "# Calculating TF-IDF scores\n",
    "tfidf_scores, vectorizer = calculate_tfidf_scores(documents_df)\n",
    "\n",
    "# Function to rank documents based on their relevance to a query\n",
    "def rank_documents(query, documents_df, tfidf_scores, vectorizer):\n",
    "    # Preprocessing the query\n",
    "    processed_query = preprocess_text(query)\n",
    "\n",
    "    # Calculating the TF-IDF scores for the query\n",
    "    query_tfidf = vectorizer.transform([' '.join(processed_query)])\n",
    "\n",
    "    # Calculating the cosine similarity between the query and the documents\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_scores).flatten()\n",
    "\n",
    "    # Getting the indices of the documents sorted by their cosine similarity to the query\n",
    "    ranked_indices = cosine_similarities.argsort()[::-1]\n",
    "\n",
    "    # Getting the filenames of the documents sorted by their cosine similarity to the query\n",
    "    ranked_documents = documents_df['Filename'].iloc[ranked_indices]\n",
    "\n",
    "    return ranked_documents\n",
    "\n",
    "# Example usage:\n",
    "# ranked_documents = rank_documents('example query', documents_df, tfidf_scores, vectorizer)\n",
    "# print(ranked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48         mcgill_manfredi.txt\n",
      "31            daily_catlin.txt\n",
      "37                   mm_01.txt\n",
      "33                  cbc_02.txt\n",
      "12                   mm_05.txt\n",
      "21              ctv_lofaro.txt\n",
      "1                   cbc_01.txt\n",
      "29        gazette_petition.txt\n",
      "17           the_challenge.txt\n",
      "27        the_stakeholders.txt\n",
      "14                globe_01.txt\n",
      "20         city_madocjones.txt\n",
      "7             globe_amador.txt\n",
      "13        global_jelowicki.txt\n",
      "0        tribune_grewal_01.txt\n",
      "39          gazette_magder.txt\n",
      "24        global_carpenter.txt\n",
      "30          tribune_wexler.txt\n",
      "8                 the_site.txt\n",
      "22        aptn_fournier_01.txt\n",
      "19             the_mission.txt\n",
      "11            cbc_lapierre.txt\n",
      "32             ed_cable_01.txt\n",
      "44                   mm_06.txt\n",
      "41                     faq.txt\n",
      "10                  cbc_03.txt\n",
      "4                mm_sep_12.txt\n",
      "43                timeline.txt\n",
      "38          tribune_grewal.txt\n",
      "40                   mm_03.txt\n",
      "46        aptn_fournier_03.txt\n",
      "25             mm_petition.txt\n",
      "15        aptn_fournier_02.txt\n",
      "35             the_site_01.txt\n",
      "26         gazette_dunlevy.txt\n",
      "18         city_rubertucci.txt\n",
      "36           tribune_cason.txt\n",
      "5                 ed_cable.txt\n",
      "28          global_omalley.txt\n",
      "9       guiding_principles.txt\n",
      "34                   mm_02.txt\n",
      "16         gazette_tomesco.txt\n",
      "42             ed_bonspiel.txt\n",
      "23       cultural_survival.txt\n",
      "45          city_henriques.txt\n",
      "6               the_vision.txt\n",
      "3     innovation_ecosystem.txt\n",
      "2                    mm_04.txt\n",
      "47              ctv_harold.txt\n",
      "Name: Filename, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ranked_documents = rank_documents('burial site', documents_df, tfidf_scores, vectorizer)\n",
    "print(ranked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Search Function\n",
    "Implement a search function that takes a query as input and returns a list of documents sorted by their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to implement search engine\n",
    "def search(query):\n",
    "    # Rank documents based on their relevance to the query\n",
    "    ranked_documents = rank_documents(query, documents_df, tfidf_scores, vectorizer)\n",
    "    \n",
    "    # Return the list of documents sorted by their scores\n",
    "    return list(ranked_documents)\n",
    "\n",
    "# Example usage:\n",
    "# search_results = search('example query')\n",
    "# print(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mcgill_manfredi.txt', 'daily_catlin.txt', 'mm_01.txt', 'cbc_02.txt', 'mm_05.txt', 'ctv_lofaro.txt', 'cbc_01.txt', 'gazette_petition.txt', 'the_challenge.txt', 'the_stakeholders.txt', 'globe_01.txt', 'city_madocjones.txt', 'globe_amador.txt', 'global_jelowicki.txt', 'tribune_grewal_01.txt', 'gazette_magder.txt', 'global_carpenter.txt', 'tribune_wexler.txt', 'the_site.txt', 'aptn_fournier_01.txt', 'the_mission.txt', 'cbc_lapierre.txt', 'ed_cable_01.txt', 'mm_06.txt', 'faq.txt', 'cbc_03.txt', 'mm_sep_12.txt', 'timeline.txt', 'tribune_grewal.txt', 'mm_03.txt', 'aptn_fournier_03.txt', 'mm_petition.txt', 'aptn_fournier_02.txt', 'the_site_01.txt', 'gazette_dunlevy.txt', 'city_rubertucci.txt', 'tribune_cason.txt', 'ed_cable.txt', 'global_omalley.txt', 'guiding_principles.txt', 'mm_02.txt', 'gazette_tomesco.txt', 'ed_bonspiel.txt', 'cultural_survival.txt', 'city_henriques.txt', 'the_vision.txt', 'innovation_ecosystem.txt', 'mm_04.txt', 'ctv_harold.txt']\n"
     ]
    }
   ],
   "source": [
    "search_results = search(\"burial site\")\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Search Results\n",
    "Display the search results in a readable format, including the document name and its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def display_search_results(search_results):\n",
    "    for rank, filename in enumerate(search_results, start=1):\n",
    "        # Load document\n",
    "        with open(os.path.join(\"data\", filename), 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Parse YAML frontmatter\n",
    "        frontmatter, content_lines = lines[:7], lines[7:]\n",
    "        metadata = yaml.safe_load(\"\\n\".join(frontmatter))\n",
    "        \n",
    "        # Join content lines\n",
    "        content = \"\\n\".join(content_lines)\n",
    "        \n",
    "        # Print information\n",
    "        print(f\"Rank: {rank}\")\n",
    "        print(f\"Document: {content}\")\n",
    "        print(f\"Author: {metadata['author']}\")\n",
    "        print(f\"Publisher: {metadata['publisher']}\")\n",
    "        print(f\"URL: {metadata['url']}\")\n",
    "        print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1\n",
      "Document: Following up on the last update I provided about the work being conducted on the former Royal Victoria Hospital site, ground penetrating radar work was performed in priority areas of the site, as established  by the Settlement Agreement, a legally binding agreement we entered into last April with the Kanien’keha:ka Kahnistensera, also known as the Mohawk Mothers, and the Société québécoise des infrastructures (SQI), which is responsible for redeveloping the site.\n",
      "\n",
      "The company conducting the work stated in a report summarizing its findings that no “likely” grave type features was identified across the site.\n",
      "\n",
      "However, “[i]n total, nine (9) geophysical signatures were identified across the site that display attributes allowing us to categorize them as “potential” grave type features.” \n",
      "\n",
      "These nine signatures are all outside the McGill project zone, which accounts for about 15 per cent of the entire site.\n",
      "\n",
      "The SQI indicated it will follow the recommendation of the Panel ofexpert archeologists, agreed to by all three parties, to investigate thenine areas manually. \n",
      "\n",
      "This technique will verify the results of theground penetrating radar, which cannot, on its own, confirm the presenceof graves. \n",
      "\n",
      "All archeological work is being conducted in the presence ofCultural Monitors named by the Kanien’keha:ka Kahnistensera to ensureappropriate Indigenous protocols and ceremonies are respected.\n",
      "\n",
      "Meanwhile, discussions continue between the SQI, the Kanien’keha:ka Kahnistensera and the Independent Special Interlocutor for Missing Children and Unmarked Graves and Burial Sites, with respect to security on the site. \n",
      "\n",
      "This follows a reprehensible act last week, in which offensive remarks were directed at the Cultural Monitors as they were inappropriately asked to leave the site by a security guard. \n",
      "\n",
      "The SQI, which is responsible for security on the site, immediately condemned the action and dealt with the incident. \n",
      "\n",
      "Since then, discussions have been taking place to reach an agreement among the parties to put on additional security measures so that the archeological work at the site may resume as soon as possible.\n",
      "\n",
      "We have been and will continue to work in a spirit of collaboration and mutual respect with the SQI and the Kanien’keha:ka Kahnistensera that adheres to and respects the Settlement Agreement we all entered into.\n",
      "\n",
      "We will continue to keep you updated of significant events as they occur.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mdisplay_search_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 17\u001b[0m, in \u001b[0;36mdisplay_search_results\u001b[0;34m(search_results)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRank: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmetadata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPublisher: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublisher\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metadata' is not defined"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "def display_search_results(search_results):\n",
    "    for rank, filename in enumerate(search_results, start=1):\n",
    "        # Load document\n",
    "        with open(os.path.join(\"data\", filename), 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Remove YAML frontmatter\n",
    "        content_lines = lines[7:]\n",
    "        \n",
    "        # Join content lines\n",
    "        content = \"\\n\".join(content_lines)\n",
    "        \n",
    "        # Print information\n",
    "        print(f\"Rank: {rank}\")\n",
    "        print(f\"Document: {content}\")\n",
    "        print(f\"Author: {metadata['author']}\")\n",
    "        print(f\"Publisher: {metadata['publisher']}\")\n",
    "        print(f\"URL: {metadata['url']}\")\n",
    "        print(\"--------------------\")\n",
    "        \n",
    "display_search_results(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
